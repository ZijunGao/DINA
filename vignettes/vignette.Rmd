---
title: "DINA vignette"
author: 
  - name: Zijun Gao
    affiliation: Department of Statistics, Stanford University
  - name: Trevor Hastie
    affiliation: Department of Statistics and Department of Biomedical Data Science, Stanford University
date: "<small>`r Sys.Date()`</small>"
output:
  html_document:
    toc: no
    toc_depth: 3
    number_sections: yes
    toc_float:
      collapsed: no
    code_folding: show
    theme: cerulean
vignette: > 
  %\VignetteIndexEntry{LinCDE vignette} 
  %\VignetteEngine{knitr::rmarkdown} 
  %\VignetteEncoding{UTF-8}{inputenc}
description: "Examples of DINA for general responses."
---

```{r global setting, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  tidy = FALSE
)
```

In this vignette, we will introduce how to use the R package DINA for the estimation and inference of heterogeneous treatment effects (HTE) with general responses. Here general responses refer to

# HTE for general responses and DINA

In the field of causal inference, recent attention has focused on the estimation of HTE, which allows the treatment effect to depend on subject-specific features. For continuous responses, the difference in conditional means is commonly used as an estimator of HTE. However, for binary responses, count data, and survival times, no consensus of the estimand has been reached. We propose to estimate a unified quantity --- the difference in natural parameters (DINA) --- applicable for all types of responses from the exponential family and Cox model. 
The DINA estimand is commonly used in practice, convenient to model the influence of covariates on the natural parameter scale, and avoid uninteresting or complicated heterogeneity. More details can be found in the [paper](http://arxiv.org/abs/2103.04277).

The DINA package implements an estimator of DINA that is robust to the
aforementioned confounding and non-collapsibility issues. The method
is motivated by Robinson's method and R-learner proposed to deal with the conditional mean difference. The method consists of two steps:
\begin{enumerate}
	\item Estimation of nuisance functions using any flexible algorithm; 
	\item Estimation of the treatment effect with nuisance function estimators plugged in.
\end{enumerate}
The method is insensitive to inaccurate nuisance function estimators.
By separating the estimation of nuisance functions from that of DINA, we can use powerful machine-learning tools, such as random forests and neural networks, for the former task.

The DINA package also provides two ways of constructing confidence intervals. 
One method relies on the asymptotic distribution of the estimator. 
The method is computationally friendly but implicitly assumes the nuisance function estimator to be of certain precision.
Another method uses bootstrap to estimate the variance of the estimator. The method requires a larger computation budget but makes less strict assumptions. The method is perhaps more suitable when the nuisance function estimators are only moderately accurate.

We illustrate the workflow of the DINA package via toy examples. Before all, let us install and attach the DINA package. 
```{r setup}
# TODO!!! 
# library("DINA")
# to be deleted start
source("/Users/zijungao/Desktop/Research/package/DINA/R/helper.R")
source("/Users/zijungao/Desktop/Research/package/DINA/R/DINA.R")
source("/Users/zijungao/Desktop/Research/package/DINA/R/predict.DINA.R")
source("/Users/zijungao/Desktop/Research/package/DINA/R/confint.DINA.R")
source("/Users/zijungao/Desktop/Research/package/DINA/R/coef.DINA.R")
source("/Users/zijungao/Desktop/Research/package/DINA/R/summary.DINA.R")
# to be deleted end
```

We first describe the data generation mechanism. 
We consider $d = 5$ covariates independently generated from uniform $[-1,1]$. 
The treatment assignment follows a logistic regression model. The responses are sampled from the exponential family with natural parameter functions
\begin{align*}
		\begin{cases}
			\eta_0(x) = x^\top\alpha + \delta x_1 x_2, \\
			\eta_1(x) = x^\top(\alpha + \beta) + \delta x_1 x_2,
		\end{cases} 
\end{align*}
for some $\delta$ determining the magnitude of the non-linear part in $\eta_0(x)$ and $\eta_1(x)$.
In both treatment and control groups, the response models are misspecified generalized linear models, while the difference of the natural parameters $\tau(x) := \eta_1(x) - \eta_0(x)$ is always linear. We consider continuous, binary, and discrete responses generated from Gaussian, Bernoulli, and Poisson distributions, respectively. 


# Gaussian response

To begin with, we experiment with Gaussian responses. We generate a dataset of $1000$ observations. We set _family = "gaussian"_ so that the response given the covariates is Gaussian. We set the variance of the errors to be _params$sigma = 1_. 
```{r generate data}
# parameters
n = 1000 # sample size
d = 5 # number of covariates
# coefficients
alpha = beta = theta = rep(0, d+1)
alpha[1:3] = c(-0.5, -0.5, -0.5) # coefficients of the linear part in eta_0(x)
beta[1:3] = c(0.8, 0.7, 0.6); beta[d+1] = 1 # coefficients of tau(x)
theta[1:2] = c(1, -1) # coefficients of e(x) following the logistic regression model 
delta = 0.5 # magnitude of the non-linear part in eta_0(x)

# data generation
set.seed(100)
params = list(); params$sigma = 1 # variance of the Gaussian error
data.gaussian = dataGnr(n = n, d = d, alpha = alpha, beta = beta, theta = theta, delta = delta, family = "gaussian", params = params)
```


## Training

We use the function _DINA_ to estimate the HTE. For continuous responses, the target estimand is the difference in the conditional expectation. We set the argument _family_ as "gaussian".


```{r train.gaussian, eval = F}
set.seed(100)
model.gaussian = DINA(data = data.gaussian, family = "gaussian")
```

```{r train.gaussian.hidden, echo = F}
# save
# saveRDS(model.gaussian, "./data/model.gaussian.default.rds")
# read
model.gaussian = readRDS("./data/model.gaussian.default.rds")
```

We print the estimated coefficients. The true coefficient is $1$ for the intercept and $[0.8, 0.7, 0.6, 0, 0]$ for $X_1$ to $X_5$.
```{r coefficients}
coef(model.gaussian) 
```

We also display the $95\%$ confidence intervals of the estimates. In this example, the confidence intervals of all coefficients cover the corresponding true values.
```{r confidence.interval}
confint(model.gaussian, level = 0.95) 
```

We can also call _summary_ to extract the estimated coefficients, standard deviations, and p-values. 
```{r summary}
summary(model.gaussian)
```

We plot the predicted HTE against the truth. The predicted values are reasonably precise. 
```{r predict}
predicted.HTE = predict(model.gaussian, newdata = data.gaussian$X)
plot(data.gaussian$tau, predicted.HTE, pch = 16, main = "Continuous responses", xlab = "true values", ylab = "predicted values")
abline(a = 0, b = 1, col = "red", lwd = 3)
```

We discuss a few arguments of the function _DINA_. More details can be found by calling _help(DINA)_.

* _prop_: method for estimating the propensity score. _Prop_ can be a character string naming the estimator: "logistic regression", "gradient boosting". _Prop_ can also be a function and _prop(X)_ should output the estimated propensity scores for each row of the covariate matrix $X$. Default is "logistic regression". Parameters of the propensity score estimation can be provided to the estimator in the argument _paramsW_.

* _baseline_: method for estimating the natural parameter functions in the treatment and control groups. _Baseline_ can be a character string naming the estimator: "regression", "gradient boosting". Parameters of the natural parameter function estimation can be provided to the estimator in the argument _params_. _Baseline_ can also be a list of functions corresponding to the estimated natural parameters. The functions should take the covariate vector of an observation (or a covariate matrix, each row represents an observation) and output the estimated natural parameters. 

Below we fit another model where we learn the natural parameter functions by gradient boosting. _DINA_ uses the R package _xgboost_ for the gradient boosting. 

```{r nuisance.estimation, eval = F} 
params$nrounds = 200; params$params = list(eta = 0.1) # parameters for the function xgboost in the R package xgboost. See the documentation of the function xgboost for more details. 
set.seed(100)
model.gaussian.boosting = DINA(data = data.gaussian, family = "gaussian", baseline = "gradient boosting", params = params)
```


```{r nuisance.estimation.hidden, echo = F}
# save
# saveRDS(model.gaussian.boosting, "./data/model.gaussian.boosting.rds")
# read
model.gaussian.boosting = readRDS("./data/model.gaussian.boosting.rds")
```

* _k_: the number of folds for cross-fitting. Default is 2. In cross-fitting, the data is divided into $k$ folds of equal size. Each time, $k-1$ folds are used for the nuisance function estimation and the left-out fold is used for the HTE estimation. This procedure is repeated for all $k$ folds. A smaller $k$ is more computationally friendly. However, if the nuisance function learners are data-intensive, we recommend using a larger $k$ such as $5$. In this way, the nuisance function learners are obtained from a larger proportion ($1-1/k$) of the training dataset. 

<!-- Below we experiment with $k = 2$, $5$, and $10$ folds. We evaluate the estimation error on a separate test dataset. We observe that $k = 10$ yields the best performance. An explanation is that the nuisance function estimator _xgboost_ is data-intensive and should be applied to a larger proportion ($1-1/k$) of the training dataset. -->

* _variance.method_: method for estimating the variance matrix. \code{Method} should be a character string naming the estimator. Current options are "CLT" and "bootstrap". Default is "CLT". 
If "bootstrap" is used for computing the variance estimator, the parameter _B_ specifies the number of bootstrap resamples. Default is $100$.
When the training sample size is small or the nuisance function estimators are variant, we recommend using "bootstrap" for inference. The method "CLT" does not account for the variance of the nuisance function estimators and may yield significantly downward-biased variance estimates here.


# Exponential family responses

Next, we move from Gaussian responses to more general responses from the exponential family.

If the response is binary, we set _family = "binomial"_. The estimation and inference procedures are the same as those for _family = "gaussian"_.

```{r binomial, eval = F}
set.seed(100)
# data generation
data.binomial = dataGnr(n = n, d = d, alpha = alpha, beta = beta, theta = theta, delta = delta, family = "binomial", params = params)

# estimation
model.binomial = DINA(data = data.binomial, family = "binomial", variance.method = "CLT")

# inference
confint(model.binomial)
```

```{r binomial.hidden, echo = F}
# save
# saveRDS(model.binomial, "./data/model.binomial.rds")
# read
model.binomial = readRDS("./data/model.binomial.rds")
confint(model.binomial)
```

If the responses follow a Poisson distribution, we set _family = "poisson"_. The estimation and inference procedures are the same as those for _family = "gaussian"_.

```{r poisson, eval = F}
set.seed(100) 
# data generation
data.poisson = dataGnr(n = n, d = d, alpha = alpha, beta = beta, theta = theta, delta = delta, family = "poisson", params = params)

# estimation
model.poisson = DINA(data = data.poisson, family = "poisson") 

# inference
confint(model.poisson)
```

```{r poisson.hidden, echo = F}
# save
# saveRDS(model.poisson, "./data/model.poisson.rds")
# read
model.poisson = readRDS("./data/model.poisson.rds")
confint(model.poisson)
```


# Survival time response

In this section, we consider survival time responses under the Cox model. The treatment effect is quantified in the log-hazard scale. We set _family = "cox"_. The estimation and inference procedures are the same as those for _family = "gaussian"_. 
  
```{r cox, eval = F}
set.seed(100) 
# data generation
data.cox = dataGnr(n = n, d = d, alpha = alpha, beta = beta, theta = theta, delta = delta, family = "cox", params = params)

# estimation
model.cox = DINA(data = data.cox, family = "cox")

# inference
confint(model.cox)
```

```{r cox.hidden, echo = F}
# save
# saveRDS(model.cox, "./data/model.cox.rds")
# read
model.cox = readRDS("./data/model.cox.rds")
# inference
confint(model.cox)
```

In survival analysis, it is common to observe censored data: the type of missing data in which the time to event is not observed. Various reasons will lead to censoring, for example, a subject may leave the study prior to experiencing an event.

_DINA_ is able to handle right-censored data with known or unknown censoring mechanisms. 
Below is an example with known censoring mechanism _censorType = "fixed"_: there is a known fixed termination time and subjects who have not experienced any event until the termination time are censored. About $25\%$ of the subjects are censored in the example.
We use the Cox regression ( _baseline = "regression"_ ) as the nuisance function estimator.


```{r cox.censoring, eval = F} 
set.seed(100) 
params$censorType = "fixed"; params$T0 = 1 # censoring mechanism: there is a known fixed termination time T0 = 1 and subjects who have not experienced any event until the termination time are censored.

# data generation
data.cox.fixed = dataGnr(n = n, d = d, alpha = alpha, beta = beta, theta = theta,  typeMisMu0 = "interaction", delta = delta, family = "cox", params = params)

# estimation
model.cox.fixed = DINA(data = data.cox.fixed, family = "cox", baseline = "regression", params = params)

# inference
confint(model.cox.fixed)
```
  
```{r cox.censoring.hidden, echo = F}
# save
# saveRDS(model.cox.fixed, "./data/model.cox.fixed.rds")
# read
model.cox.fixed = readRDS("./data/model.cox.fixed.rds")
# inference
confint(model.cox.fixed)
```

If the censoring mechanism is unknown, we set _censorType = "unknown"_. 
_DINA_ will first learn the probability of not being censored by fitting a classification model with the censoring indicator as the response and the covariates $X$ as the predictors. 
The learnt censored probabilities are provided to the follow-up nuisance function construction.
Below we refit the model with _censorType = "unknown"_, and the results are comparable to those obtained knowing the true censoring mechanism. 

```{r cox.censoring.unknown, eval = F} 
set.seed(100) 
params$censorType = "unknown"

# estimation
model.cox.unknown = DINA(data = data.cox.fixed, family = "cox", baseline = "regression", params = params)

# inference
confint(model.cox.unknown)
```

```{r cox.censoring.unknown (hidden), echo = F}
# save
# saveRDS(model.cox.unknown, "./data/model.cox.unknown.rds")
# read
model.cox.unknown = readRDS("./data/model.cox.unknown.rds")
# inference
confint(model.cox.unknown)
```

# Bagging

In this section, we discuss an ensemble of _DINA_ through bootstrap aggregation (bagging).The bagging estimator possesses better estimation and inference performance. 

* _Estimation_.
Since _DINA_ uses cross-fitting, the causal estimator will thus depend, perhaps heavily, on the realization of the sample splitting.
The dependence may be stronger if the sample size is small or the nuisance function learners are variant.
Bootstrap aggregation can remove the randomness by repeating the sample splitting multiple times and average the associated estimators.

* _Inference_. Stefan et.al. develop a bootstrap-type standard deviation estimator for general bagging estimators. 
The approach uses the pre-existing bootstrap samples of bagging and does not require an additional round of bootstrapping. 
The bootstrap-type standard deviation estimator is data adaptive.

We use a toy example to demonstrate the advantages of the bagging estimator.

We use a smaller sample size $n = 200$ with Gaussian responses.
All the rest of the data generation mechanism is the same as before.
We carry out the standard DINA estimation and compute the bagging estimator with $B = 400$ bootstrap resamples. 

```{r bagging.gaussian, eval = F}
set.seed(100)
data.gaussian.small = dataGnr(n = 200, d = d, alpha = alpha, beta = beta, theta = theta, delta = delta, family = "gaussian", params = params)
model.gaussian.bagging = DINA.bagging(data = data.gaussian.small, family = "gaussian", B = 400) # bagged DINA with B = 400 bootstrap resamples
```

```{r bagging.gaussian.2, eval = F, echo = F}
parameters.true = c(rev(beta)[1], rev(rev(beta)[-1]))

set.seed(100) 
m = 100
coverage = list(); coverage$default = coverage$bootstrap = coverage$bagging = matrix(0, m, length(parameters.true))
estimator = list(); estimator$default = estimator$bootstrap = estimator$bagging = matrix(0, m, length(parameters.true))

width = list(); width$default = width$bootstrap = width$bagging = matrix(0, m, length(parameters.true))
params$params = list(eta = 0.9); params$nrounds = 100

for(l in 1:m){
  data.gaussian.small = dataGnr(n = 200, d = d, alpha = alpha, beta = beta, theta = theta,  typeMisMu0 = "interaction", delta = delta, family = "gaussian", params = params)
  
  set.seed(100 * l)
  model.gaussian.default.small = DINA(data = data.gaussian.small, family = "gaussian", params = params)
  estimator$default[l,] = coef(model.gaussian.default.small)
  coverage$default[l,] = pmin((confint(model.gaussian.default.small)[,1] < parameters.true), (confint(model.gaussian.default.small)[,2] > parameters.true))
  width$default[l,] = confint(model.gaussian.default.small)[,2] - confint(model.gaussian.default.small)[,1]
  
  set.seed(100 * l)
  model.gaussian.bootstrap.small = DINA(data = data.gaussian.small, family = "gaussian", variance.method = "bootstrap", B = 400, params = params)
  estimator$bootstrap[l,] = coef(model.gaussian.bootstrap.small)
  coverage$bootstrap[l,] = pmin((confint(model.gaussian.bootstrap.small)[,1] < parameters.true), (confint(model.gaussian.bootstrap.small)[,2] > parameters.true))
  width$bootstrap[l,] = confint(model.gaussian.bootstrap.small)[,2] - confint(model.gaussian.bootstrap.small)[,1]
  
  set.seed(100 * l)
  model.gaussian.bagging.small = DINA.bagging(data = data.gaussian.small, family = "gaussian", B = 400, params = params)
  estimator$bagging[l,] = coef(model.gaussian.bagging.small)
  coverage$bagging[l,] = pmin((confint(model.gaussian.bagging.small)[,1] < parameters.true), (confint(model.gaussian.bagging.small)[,2] > parameters.true))
  width$bagging[l,] = confint(model.gaussian.bagging.small)[,2] - confint(model.gaussian.bagging.small)[,1]
  
  print(l)
} 

result = list()
result$coverage = coverage
result$width = width
result$estimator = estimator
```

```{r bagging.gaussian.hidden, eval = F}
# save
# saveRDS(result, "./data/bagging.gaussian.rds")
# read
result = readRDS("./data/bagging.gaussian.rds")
```

We repeat from data generation $100$ times, and overlay the histograms of the estimated intercepts with the true value (in red) superimposed. 

```{r histogram, echo = F}
hist.xlim = range(c(result$estimator$default[,1], result$estimator$bagging[,1])) + c(-0.2, 0.2)
p1 = hist(result$estimator$default[,1], 
     breaks = seq(hist.xlim[1], hist.xlim[2], length.out = 15), 
     plot = F)
p2 = hist(result$estimator$bagging[,1], 
     breaks = seq(hist.xlim[1], hist.xlim[2], length.out = 15),
     plot = F)

plot(p1, col=rgb(0,0,0,1/4), xlim=hist.xlim, ylim = c(0,3), freq = F, xlab = "estimator", main = "")  # first histogram
plot(p2, col=rgb(0,0,1,1/4), xlim=hist.xlim, freq = F, add=T)  # second
abline(v = 1, col = "red", lwd = 2)
legend("topright", legend = c("DINA", "bagged DINA"), col = c(rgb(0,0,0,1/4), rgb(0,0,1,1/4)), lwd = 10)
help("hist")
```

Both the standard DINA and the bagging estimator are approximately unbiased. 
The bagging estimator has a smaller spread, i.e., the bootstrap aggregation helps to remove part of the variance.

We next evaluate the coverage of the constructed $95\%$ confidence intervals.
For the standard DINA estimator, we consider two variance estimators: _variance.method = "CLT"_ and  _variance.method = "bootstrap"_.
For the bagging estimator, we use the default variance estimator:  _variance.method = "infinitesimal jackknife"_ with bias correction _variance.bias.correction = T_.

We plot the confidence intervals of the intercept obtained by the two methods across $100$ trials.
We plot the confidence intervals increasingly in the point estimates. We color the confidence intervals failing to cover the true intercept, i.e., $1$, in red. 

```{r bagging.gaussian.plot.data, echo = F}
m = length(result$coverage$default[,1])

bagging.plot.data = data.frame(method = rep(c("DINA", "bootstrap", "bagging"), c(m, m, m)))
bagging.plot.data$coverage = c(result$coverage$default[,1],
                               result$coverage$bootstrap[,1],
                               result$coverage$bagging[,1])
bagging.plot.data$width = c(result$width$default[,1],
                            result$width$bootstrap[,1],
                            result$width$bagging[,1])
bagging.plot.data$estimator = c(result$estimator$default[,1],
                                result$estimator$bootstrap[,1],
                                result$estimator$bagging[,1])
bagging.plot.data$upper.CI = bagging.plot.data$estimator + bagging.plot.data$width/2
bagging.plot.data$lower.CI = bagging.plot.data$estimator - bagging.plot.data$width/2 
bagging.plot.data$cover = abs(bagging.plot.data$estimator - rev(beta)[1]) < (bagging.plot.data$width/2)
```

```{r bagging.gaussian.confidence.interval.plot, echo = F}
# DINA + CLT
par(mfrow = c(1,3))
plot(rank(bagging.plot.data$estimator[which(bagging.plot.data$method == "DINA")]), 
     bagging.plot.data$estimator[which(bagging.plot.data$method == "DINA")], 
     ylim = c(min(bagging.plot.data$lower.CI), max(bagging.plot.data$upper.CI)), 
     pch=19, 
     col=ifelse(bagging.plot.data$cover[which(bagging.plot.data$method == "DINA")], "grey", "red"),
     xlab= "", 
     ylab= "95% confidence intervals (CLT)", 
     main = "DINA + CLT")
segments(rank(bagging.plot.data$estimator[which(bagging.plot.data$method == "DINA")]), 
         y0 = bagging.plot.data$lower.CI[which(bagging.plot.data$method == "DINA")], 
         y1 = bagging.plot.data$upper.CI[which(bagging.plot.data$method == "DINA")], 
         col=ifelse(bagging.plot.data$cover[which(bagging.plot.data$method == "DINA")],"grey","red"))
abline(h = 1, lty = 2)

# DINA + bootstrap
plot(rank(bagging.plot.data$estimator[which(bagging.plot.data$method == "bootstrap")]), 
     bagging.plot.data$estimator[which(bagging.plot.data$method == "bootstrap")], 
     ylim = c(min(bagging.plot.data$lower.CI), max(bagging.plot.data$upper.CI)), 
     pch=19, 
     col=ifelse(bagging.plot.data$cover[which(bagging.plot.data$method == "bootstrap")], "grey", "red"),
     xlab= "", 
     ylab= "95% confidence intervals (CLT)", 
     main = "DINA + bootstrap")
segments(rank(bagging.plot.data$estimator[which(bagging.plot.data$method == "bootstrap")]), 
         y0 = bagging.plot.data$lower.CI[which(bagging.plot.data$method == "bootstrap")], 
         y1 = bagging.plot.data$upper.CI[which(bagging.plot.data$method == "bootstrap")], 
         col=ifelse(bagging.plot.data$cover[which(bagging.plot.data$method == "bootstrap")],"grey","red"))
abline(h = 1, lty = 2)

# bagging
plot(rank(bagging.plot.data$estimator[which(bagging.plot.data$method == "bagging")]), 
     bagging.plot.data$estimator[which(bagging.plot.data$method == "bagging")], 
     ylim = c(min(bagging.plot.data$lower.CI), max(bagging.plot.data$upper.CI)), 
     pch=19, 
     col=ifelse(bagging.plot.data$cover[which(bagging.plot.data$method == "bagging")], "grey", "red"),
     ylab = "95% confidence intervals", 
     xlab= "", 
     main = "Bagging (IJ)")
segments(rank(bagging.plot.data$estimator[which(bagging.plot.data$method == "bagging")]), 
         y0 = bagging.plot.data$lower.CI[which(bagging.plot.data$method == "bagging")], 
         y1 = bagging.plot.data$upper.CI[which(bagging.plot.data$method == "bagging")], 
         col=ifelse(bagging.plot.data$cover[which(bagging.plot.data$method == "bagging")],"grey","red"))
abline(h = 1, lty = 2)
```

For the standard DINA estimator with _variance.method = "CLT"_, the coverage falls slightly below the target value ($82$ covered out of $100$ trials). 
For the standard DINA estimator with _variance.method = "bootstrap"_ and the bagging DINA estimator with _variance.method = "infinitesimal jackknife"_, _variance.bias.correction = T_, the coverages $89\%$, $93\%$ are closer to the target $95\%$.

In addition, the confidence interval widths of the bagging estimator are about $22\%$ shorter than those of the standard DINA estimator with _variance.method = "CLT"_, and $27\%$ shorter than those of the standard DINA estimator with _variance.method = "bootstrap"_.


## Plug in your own data and have fun with DINA!



